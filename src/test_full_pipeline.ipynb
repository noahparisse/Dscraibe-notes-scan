{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a68b0c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Ajoute la racine du projet (celle qui contient 'src') au sys.path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adbc0b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.image_utils import encode_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0efefc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing.mistral_ocr_llm import image_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1379f15f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OCRPageObject' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m image_path = \u001b[33m\"\u001b[39m\u001b[33m/Users/tomamirault/Documents/projects/p1-dty-rte/detection-notes/tmp/paper/detection_20251015-171658-182_q0.jpg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m base64_image = encode_image(image_path)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mimage_transcription\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase64_image\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/p1-dty-rte/detection-notes/src/processing/mistral_ocr_llm.py:94\u001b[39m, in \u001b[36mimage_transcription\u001b[39m\u001b[34m(base64_image)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mimage_transcription\u001b[39m(base64_image: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# 1. OCR brut\u001b[39;00m\n\u001b[32m     86\u001b[39m     response = client.ocr.process(\n\u001b[32m     87\u001b[39m         model=\u001b[33m\"\u001b[39m\u001b[33mmistral-ocr-latest\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     88\u001b[39m         document={\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m         include_image_base64=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     93\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     ocr_text = \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpages\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m.strip()\n\u001b[32m     95\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== OCR brut ===\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, ocr_text)\n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# 2. Pré-process\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/p1-dty-rte/detection-notes/.venv/lib/python3.13/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'OCRPageObject' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "image_path = \"/Users/tomamirault/Documents/projects/p1-dty-rte/detection-notes/tmp/paper/detection_20251015-171658-182_q0.jpg\"\n",
    "base64_image = encode_image(image_path)\n",
    "image_transcription(base64_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "997c6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing.add_data2db import add_data2db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e800291e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV\n",
      "- Ancienne ligne 3. MNV à réaliser SUAV\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV'}, {'type': 'delete', 'old_line': 3, 'old_content': 'MNV à réaliser SUAV'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 4)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV\n",
      "- Ancienne ligne 3. MNV à réaliser SUAV\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV'}, {'type': 'delete', 'old_line': 3, 'old_content': 'MNV à réaliser SUAV'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV\n",
      "- Ancienne ligne 3. MNV à réaliser SUAV\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV'}, {'type': 'delete', 'old_line': 3, 'old_content': 'MNV à réaliser SUAV'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 4)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV\n",
      "- Ancienne ligne 3. MNV à réaliser SUAV\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes: valeurs alarmantes de 500 MV'}, {'type': 'delete', 'old_line': 3, 'old_content': 'MNV à réaliser SUAV'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data2db(\"/Users/tomamirault/Documents/projects/p1-dty-rte/detection-notes/data/images_scenarios/IMG_3416.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c14e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 5)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 5)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen-Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data2db(\"/Users/tomamirault/Documents/projects/p1-dty-rte/detection-notes/data/images_scenarios/IMG_3417.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0daf7214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "+ Ligne 3. MNV à réaliser SUAV\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV'}, {'type': 'insert', 'line': 3, 'content': 'MNV à réaliser SUAV'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 6)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "+ Ligne 3. MNV à réaliser SUAV\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV'}, {'type': 'insert', 'line': 3, 'content': 'MNV à réaliser SUAV'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV\n",
      "=== Après pré-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "+ Ligne 3. MNV à réaliser SUAV\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV'}, {'type': 'insert', 'line': 3, 'content': 'MNV à réaliser SUAV'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 6)\n",
      "=== Réponse LLM brute ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== Après post-process ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "\n",
      "=== OCR brut ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes:\n",
      "Valeurs alarmantes de 500 MV\n",
      "$\\rightarrow$ MNV à réaliser SUAV \n",
      "\n",
      "=== Normalisé ===\n",
      " Prévoir retrait de la liaison Caen - Cherbourg\n",
      "Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "MNV à réaliser SUAV\n",
      "=== DIFF HUMAIN ===\n",
      "+ Ligne 1. Prévoir retrait de la liaison Caen - Cherbourg\n",
      "+ Ligne 2. Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV\n",
      "+ Ligne 3. MNV à réaliser SUAV\n",
      "- Ancienne ligne 1. Prévoir retrait de la liaison Caen-Cherbourg\n",
      "- Ancienne ligne 2. Surveiller conso entre Nantes et Vannes\n",
      "=== DIFF JSON ===\n",
      "[{'type': 'insert', 'line': 1, 'content': 'Prévoir retrait de la liaison Caen - Cherbourg'}, {'type': 'insert', 'line': 2, 'content': 'Surveiller conso entre Nantes et Vannes : valeurs alarmantes de 500 MV'}, {'type': 'insert', 'line': 3, 'content': 'MNV à réaliser SUAV'}, {'type': 'delete', 'old_line': 1, 'old_content': 'Prévoir retrait de la liaison Caen-Cherbourg'}, {'type': 'delete', 'old_line': 2, 'old_content': 'Surveiller conso entre Nantes et Vannes'}]\n",
      "Nouvelle version pour la note existante 660652f6-1bbc-4a8a-b895-064fdd783099\n",
      "Note insérée (note_id 660652f6-1bbc-4a8a-b895-064fdd783099, meta_id 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_data2db(\"/Users/tomamirault/Documents/projects/p1-dty-rte/detection-notes/data/images_scenarios/IMG_3418.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63db2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "def _normalize_for_similarity(s: str) -> str:\n",
    "    # Minimise l'effet de variations typographiques mineures\n",
    "    s = s.strip().lower()\n",
    "\n",
    "    # Normalise les tirets et espaces autour des signes - : ; , .\n",
    "    s = re.sub(r\"\\s*[-–—]\\s*\", \"-\", s)       # \"Caen - Cherbourg\" -> \"caen-cherbourg\"\n",
    "    s = re.sub(r\"\\s*:\\s*\", \":\", s)\n",
    "    s = re.sub(r\"\\s*;\\s*\", \";\", s)\n",
    "    s = re.sub(r\"\\s*,\\s*\", \",\", s)\n",
    "    s = re.sub(r\"\\s*\\.\\s*\", \".\", s)\n",
    "\n",
    "    # Écrase espaces multiples\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _similarity(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, _normalize_for_similarity(a), _normalize_for_similarity(b)).ratio()\n",
    "\n",
    "def _align_block(old_block: List[str], new_block: List[str]) -> Tuple[List[Tuple[int,int,float]], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Aligne un bloc old_block (indexés par i) et new_block (indexés par j) par similarité.\n",
    "    Retourne:\n",
    "      - matches: liste de (i, j, sim) appariés (greedy, sim décroissante)\n",
    "      - old_unmatched: indices i non appariés (à supprimer)\n",
    "      - new_unmatched: indices j non appariés (à insérer)\n",
    "    \"\"\"\n",
    "    # calcule toutes les paires avec leur similarité\n",
    "    pairs = []\n",
    "    for i, a in enumerate(old_block):\n",
    "        for j, b in enumerate(new_block):\n",
    "            sim = _similarity(a, b)\n",
    "            pairs.append((sim, i, j))\n",
    "    # tri décroissant par similarité\n",
    "    pairs.sort(reverse=True, key=lambda t: t[0])\n",
    "\n",
    "    matched_old = set()\n",
    "    matched_new = set()\n",
    "    matches = []\n",
    "\n",
    "    for sim, i, j in pairs:\n",
    "        if i in matched_old or j in matched_new:\n",
    "            continue\n",
    "        # On accepte un match même si sim est faible ; on décidera ensuite s’il faut le logguer\n",
    "        matched_old.add(i)\n",
    "        matched_new.add(j)\n",
    "        matches.append((i, j, sim))\n",
    "\n",
    "    old_unmatched = [i for i in range(len(old_block)) if i not in matched_old]\n",
    "    new_unmatched = [j for j in range(len(new_block)) if j not in matched_new]\n",
    "    # tri par indices croissants pour stabilité\n",
    "    matches.sort(key=lambda t: (t[1], t[0]), reverse=False)  # principalement par j (ordre des nouvelles lignes)\n",
    "    return matches, old_unmatched, new_unmatched\n",
    "\n",
    "def compute_diff(old_text: str,\n",
    "                 new_text: str,\n",
    "                 minor_change_threshold: float = 0.90) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Renvoie (human_str, diff_json)\n",
    "    - human_str : lignes ajoutées / modifiées / supprimées (monospace) avec n° de ligne\n",
    "    - diff_json : liste d'opérations {type, line, content, ...}\n",
    "      type ∈ {\"insert\",\"replace\",\"delete\"}\n",
    "      line = n° de ligne dans le NOUVEAU texte (1-based) pour insert/replace,\n",
    "             n° de ligne dans l’ANCIEN pour delete (clé 'old_line').\n",
    "    Règles :\n",
    "      - insert : toujours listé\n",
    "      - replace : listé seulement si différence significative (similarité < minor_change_threshold)\n",
    "      - delete : toujours listé\n",
    "    \"\"\"\n",
    "    old_lines = old_text.splitlines()\n",
    "    new_lines = new_text.splitlines()\n",
    "\n",
    "    sm = SequenceMatcher(None, old_lines, new_lines, autojunk=False)\n",
    "\n",
    "    human_rows: List[str] = []\n",
    "    diff_json: List[Dict] = []\n",
    "\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == \"equal\":\n",
    "            continue\n",
    "\n",
    "        old_block = old_lines[i1:i2]\n",
    "        new_block = new_lines[j1:j2]\n",
    "\n",
    "        if tag in (\"replace\", \"insert\", \"delete\"):\n",
    "            # Aligne intelligemment, même si tailles identiques\n",
    "            matches, old_unmatched, new_unmatched = _align_block(old_block, new_block)\n",
    "\n",
    "            # 1) REPLACE (pour les paires appariées)\n",
    "            for i_rel, j_rel, sim in matches:\n",
    "                old_content = old_block[i_rel]\n",
    "                new_content = new_block[j_rel]\n",
    "                new_abs_line = j1 + j_rel + 1      # 1-based dans le nouveau texte\n",
    "                old_abs_line = i1 + i_rel + 1      # 1-based dans l’ancien texte\n",
    "\n",
    "                # Ne journalise pas si c'est un changement mineur\n",
    "                if _normalize_for_similarity(old_content) == _normalize_for_similarity(new_content):\n",
    "                    # équivalent “tolérant” → rien\n",
    "                    continue\n",
    "\n",
    "                if sim < minor_change_threshold:\n",
    "                    human_rows.append(f\"~ Ligne {new_abs_line}. {new_content}\")\n",
    "                    diff_json.append({\n",
    "                        \"type\": \"replace\",\n",
    "                        \"line\": new_abs_line,\n",
    "                        \"old_line\": old_abs_line,\n",
    "                        \"old_content\": old_content,\n",
    "                        \"content\": new_content,\n",
    "                        \"similarity\": float(sim)\n",
    "                    })\n",
    "                # sinon (sim >= seuil) → on considère que c'est mineur → pas de log\n",
    "\n",
    "            # 2) INSERT (nouvelles lignes non appariées)\n",
    "            for j_rel in new_unmatched:\n",
    "                new_content = new_block[j_rel]\n",
    "                if not new_content.strip():\n",
    "                    continue\n",
    "                new_abs_line = j1 + j_rel + 1\n",
    "                human_rows.append(f\"+ Ligne {new_abs_line}. {new_content}\")\n",
    "                diff_json.append({\n",
    "                    \"type\": \"insert\",\n",
    "                    \"line\": new_abs_line,\n",
    "                    \"content\": new_content\n",
    "                })\n",
    "\n",
    "            # 3) DELETE (anciennes lignes non appariées)\n",
    "            for i_rel in old_unmatched:\n",
    "                old_content = old_block[i_rel]\n",
    "                if not old_content.strip():\n",
    "                    continue\n",
    "                old_abs_line = i1 + i_rel + 1\n",
    "                human_rows.append(f\"- Ancienne ligne {old_abs_line}. {old_content}\")\n",
    "                diff_json.append({\n",
    "                    \"type\": \"delete\",\n",
    "                    \"old_line\": old_abs_line,\n",
    "                    \"old_content\": old_content\n",
    "                })\n",
    "\n",
    "    human_str = \"\\n\".join(human_rows)\n",
    "    return human_str, diff_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c1c2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "# ----------------------------- #\n",
    "#  Utils similarité / matching  #\n",
    "# ----------------------------- #\n",
    "\n",
    "def _normalize_for_similarity(s: str) -> str:\n",
    "    # Minimise l'effet de variations typographiques mineures\n",
    "    s = s.strip().lower()\n",
    "\n",
    "    # Normalise les tirets et espaces autour des signes - : ; , .\n",
    "    s = re.sub(r\"\\s*[-–—]\\s*\", \"-\", s)       # \"Caen - Cherbourg\" -> \"caen-cherbourg\"\n",
    "    s = re.sub(r\"\\s*:\\s*\", \":\", s)\n",
    "    s = re.sub(r\"\\s*;\\s*\", \";\", s)\n",
    "    s = re.sub(r\"\\s*,\\s*\", \",\", s)\n",
    "    s = re.sub(r\"\\s*\\.\\s*\", \".\", s)\n",
    "\n",
    "    # Écrase espaces multiples\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def _similarity(a: str, b: str) -> float:\n",
    "    return SequenceMatcher(None, _normalize_for_similarity(a), _normalize_for_similarity(b)).ratio()\n",
    "\n",
    "def _similarity_concat(a: str, b1: str, b2: str) -> float:\n",
    "    return _similarity(a, f\"{b1} {b2}\".strip())\n",
    "\n",
    "def _align_block(old_block: List[str], new_block: List[str]) -> Tuple[List[Tuple[int,int,float]], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Aligne un bloc old_block (indexés par i) et new_block (indexés par j) par similarité.\n",
    "    Retourne:\n",
    "      - matches: liste de (i, j, sim) appariés (greedy, sim décroissante)\n",
    "      - old_unmatched: indices i non appariés (à supprimer)\n",
    "      - new_unmatched: indices j non appariés (à insérer)\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i, a in enumerate(old_block):\n",
    "        for j, b in enumerate(new_block):\n",
    "            sim = _similarity(a, b)\n",
    "            pairs.append((sim, i, j))\n",
    "    pairs.sort(reverse=True, key=lambda t: t[0])\n",
    "\n",
    "    matched_old = set()\n",
    "    matched_new = set()\n",
    "    matches = []\n",
    "    for sim, i, j in pairs:\n",
    "        if i in matched_old or j in matched_new:\n",
    "            continue\n",
    "        matched_old.add(i)\n",
    "        matched_new.add(j)\n",
    "        matches.append((i, j, sim))\n",
    "\n",
    "    old_unmatched = [i for i in range(len(old_block)) if i not in matched_old]\n",
    "    new_unmatched = [j for j in range(len(new_block)) if j not in matched_new]\n",
    "\n",
    "    # tri par ordre d'apparition côté NEW pour stabilité\n",
    "    matches.sort(key=lambda t: (t[1], t[0]))\n",
    "    return matches, old_unmatched, new_unmatched\n",
    "\n",
    "def _try_split_merge_matches(old_block: List[str], new_block: List[str], split_thresh: float = 0.85):\n",
    "    \"\"\"\n",
    "    Détecte localement des splits (1 old -> 2 new) et merges (2 old -> 1 new).\n",
    "    Retourne:\n",
    "      - matches: liste de tuples ((i_start,i_end), (j_start,j_end), kind) avec kind in {\"1to2\",\"2to1\"}\n",
    "      - used_old: set d'indices old consommés\n",
    "      - used_new: set d'indices new consommés\n",
    "    \"\"\"\n",
    "    n_old, n_new = len(old_block), len(new_block)\n",
    "    used_old, used_new = set(), set()\n",
    "    matches = []\n",
    "\n",
    "    # --- 1 -> 2 (split)\n",
    "    for i in range(n_old):\n",
    "        if i in used_old:\n",
    "            continue\n",
    "        best = None\n",
    "        for j in range(n_new - 1):\n",
    "            if j in used_new or (j + 1) in used_new:\n",
    "                continue\n",
    "            sim = _similarity_concat(old_block[i], new_block[j], new_block[j + 1])\n",
    "            if best is None or sim > best[0]:\n",
    "                best = (sim, i, j)\n",
    "        if best and best[0] >= split_thresh:\n",
    "            sim, i0, j0 = best\n",
    "            matches.append(((i0, i0), (j0, j0 + 1), \"1to2\"))\n",
    "            used_old.add(i0)\n",
    "            used_new.update({j0, j0 + 1})\n",
    "\n",
    "    # --- 2 -> 1 (merge)\n",
    "    for j in range(n_new):\n",
    "        if j in used_new:\n",
    "            continue\n",
    "        best = None\n",
    "        for i in range(n_old - 1):\n",
    "            if i in used_old or (i + 1) in used_old:\n",
    "                continue\n",
    "            sim = _similarity_concat(new_block[j], old_block[i], old_block[i + 1])\n",
    "            if best is None or sim > best[0]:\n",
    "                best = (sim, i, j)\n",
    "        if best and best[0] >= split_thresh:\n",
    "            sim, i0, j0 = best\n",
    "            matches.append(((i0, i0 + 1), (j0, j0), \"2to1\"))\n",
    "            used_old.update({i0, i0 + 1})\n",
    "            used_new.add(j0)\n",
    "\n",
    "    matches.sort(key=lambda m: m[1][0])  # ordre d’apparition côté NEW\n",
    "    return matches, used_old, used_new\n",
    "\n",
    "# ----------------------------- #\n",
    "#            DIFF               #\n",
    "# ----------------------------- #\n",
    "\n",
    "def compute_diff(old_text: str,\n",
    "                 new_text: str,\n",
    "                 minor_change_threshold: float = 0.90,\n",
    "                 split_merge_threshold: float = 0.85) -> Tuple[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Renvoie (human_str, diff_json)\n",
    "    - human_str : lignes ajoutées / modifiées / supprimées (monospace) avec n° de ligne\n",
    "    - diff_json : liste d'opérations {type, line, content, ...}\n",
    "      type ∈ {\"insert\",\"replace\",\"delete\"}\n",
    "      line = n° de ligne dans le NOUVEAU texte (1-based) pour insert/replace,\n",
    "             n° de ligne dans l’ANCIEN pour delete (clé 'old_line').\n",
    "    Règles :\n",
    "      - insert : toujours listé\n",
    "      - replace : listé seulement si différence significative (similarité < minor_change_threshold)\n",
    "      - delete : toujours listé\n",
    "      - split/merge : tentatives avant alignement 1↔1 pour éviter des inserts artificiels\n",
    "    \"\"\"\n",
    "    old_lines = old_text.splitlines()\n",
    "    new_lines = new_text.splitlines()\n",
    "\n",
    "    sm = SequenceMatcher(None, old_lines, new_lines, autojunk=False)\n",
    "\n",
    "    human_rows: List[str] = []\n",
    "    diff_json: List[Dict] = []\n",
    "\n",
    "    for tag, i1, i2, j1, j2 in sm.get_opcodes():\n",
    "        if tag == \"equal\":\n",
    "            continue\n",
    "\n",
    "        old_block = old_lines[i1:i2]\n",
    "        new_block = new_lines[j1:j2]\n",
    "\n",
    "        if tag in (\"replace\", \"insert\", \"delete\"):\n",
    "            # ---- 0) Détection split/merge en amont\n",
    "            split_matches, used_old, used_new = _try_split_merge_matches(\n",
    "                old_block, new_block, split_thresh=split_merge_threshold\n",
    "            )\n",
    "\n",
    "            # Émettre les remplacements pour les matches 1→2 et 2→1 (sans inserts/deletes)\n",
    "            for (i_start, i_end), (j_start, j_end), kind in split_matches:\n",
    "                if kind == \"1to2\":\n",
    "                    a = old_block[i_start]\n",
    "                    b1, b2 = new_block[j_start], new_block[j_start + 1]\n",
    "\n",
    "                    sim1 = _similarity(a, b1)\n",
    "                    if _normalize_for_similarity(a) != _normalize_for_similarity(b1) and sim1 < minor_change_threshold and b1.strip():\n",
    "                        line_new = j1 + j_start + 1\n",
    "                        human_rows.append(f\"~ Ligne {line_new}. {b1}\")\n",
    "                        diff_json.append({\n",
    "                            \"type\": \"replace\",\n",
    "                            \"line\": line_new,\n",
    "                            \"old_line\": i1 + i_start + 1,\n",
    "                            \"old_content\": a,\n",
    "                            \"content\": b1,\n",
    "                            \"similarity\": float(sim1),\n",
    "                            \"note\": \"split(1→2)-part1\"\n",
    "                        })\n",
    "\n",
    "                    sim2 = _similarity(a, b2)\n",
    "                    if _normalize_for_similarity(a) != _normalize_for_similarity(b2) and sim2 < minor_change_threshold and b2.strip():\n",
    "                        line_new = j1 + j_start + 2\n",
    "                        human_rows.append(f\"~ Ligne {line_new}. {b2}\")\n",
    "                        diff_json.append({\n",
    "                            \"type\": \"replace\",\n",
    "                            \"line\": line_new,\n",
    "                            \"old_line\": i1 + i_start + 1,\n",
    "                            \"old_content\": a,\n",
    "                            \"content\": b2,\n",
    "                            \"similarity\": float(sim2),\n",
    "                            \"note\": \"split(1→2)-part2\"\n",
    "                        })\n",
    "\n",
    "                elif kind == \"2to1\":\n",
    "                    a1, a2 = old_block[i_start], old_block[i_end]\n",
    "                    b = new_block[j_start]\n",
    "                    sim = _similarity(f\"{a1} {a2}\".strip(), b)\n",
    "                    if _normalize_for_similarity(f\"{a1} {a2}\") != _normalize_for_similarity(b) and sim < minor_change_threshold and b.strip():\n",
    "                        line_new = j1 + j_start + 1\n",
    "                        human_rows.append(f\"~ Ligne {line_new}. {b}\")\n",
    "                        diff_json.append({\n",
    "                            \"type\": \"replace\",\n",
    "                            \"line\": line_new,\n",
    "                            \"old_line\": [i1 + i_start + 1, i1 + i_end + 1],\n",
    "                            \"old_content\": f\"{a1} {a2}\",\n",
    "                            \"content\": b,\n",
    "                            \"similarity\": float(sim),\n",
    "                            \"note\": \"merge(2→1)\"\n",
    "                        })\n",
    "\n",
    "            # ---- 1) Enlève ce qui a été “consommé” par split/merge\n",
    "            old_rest_map = [k for k in range(len(old_block)) if k not in used_old]\n",
    "            new_rest_map = [k for k in range(len(new_block)) if k not in used_new]\n",
    "            old_rest = [old_block[k] for k in old_rest_map]\n",
    "            new_rest = [new_block[k] for k in new_rest_map]\n",
    "\n",
    "            # ---- 2) Aligne le reste 1↔1\n",
    "            matches, old_unmatched_rel, new_unmatched_rel = _align_block(old_rest, new_rest)\n",
    "\n",
    "            # Remappe indices relatifs vers indices initiaux de block\n",
    "            remapped_matches = []\n",
    "            for i_rel2, j_rel2, sim in matches:\n",
    "                i_rel_orig = old_rest_map[i_rel2] if i_rel2 < len(old_rest_map) else None\n",
    "                j_rel_orig = new_rest_map[j_rel2] if j_rel2 < len(new_rest_map) else None\n",
    "                remapped_matches.append((i_rel_orig, j_rel_orig, sim))\n",
    "\n",
    "            old_unmatched = [old_rest_map[i] for i in old_unmatched_rel]\n",
    "            new_unmatched = [new_rest_map[j] for j in new_unmatched_rel]\n",
    "\n",
    "            # ---- 3) REPLACE pour les paires appariées restantes\n",
    "            for i_rel, j_rel, sim in remapped_matches:\n",
    "                old_content = old_block[i_rel]\n",
    "                new_content = new_block[j_rel]\n",
    "                new_abs_line = j1 + j_rel + 1\n",
    "                old_abs_line = i1 + i_rel + 1\n",
    "\n",
    "                if _normalize_for_similarity(old_content) == _normalize_for_similarity(new_content):\n",
    "                    continue  # changement mineur “tolérant”\n",
    "\n",
    "                if sim < minor_change_threshold:\n",
    "                    human_rows.append(f\"~ Ligne {new_abs_line}. {new_content}\")\n",
    "                    diff_json.append({\n",
    "                        \"type\": \"replace\",\n",
    "                        \"line\": new_abs_line,\n",
    "                        \"old_line\": old_abs_line,\n",
    "                        \"old_content\": old_content,\n",
    "                        \"content\": new_content,\n",
    "                        \"similarity\": float(sim)\n",
    "                    })\n",
    "                # sinon (sim >= seuil) => mineur => pas de log\n",
    "\n",
    "            # ---- 4) INSERT pour les nouvelles lignes non appariées\n",
    "            for j_rel in new_unmatched:\n",
    "                new_content = new_block[j_rel]\n",
    "                if not new_content.strip():\n",
    "                    continue\n",
    "                new_abs_line = j1 + j_rel + 1\n",
    "                human_rows.append(f\"+ Ligne {new_abs_line}. {new_content}\")\n",
    "                diff_json.append({\n",
    "                    \"type\": \"insert\",\n",
    "                    \"line\": new_abs_line,\n",
    "                    \"content\": new_content\n",
    "                })\n",
    "\n",
    "            # ---- 5) DELETE pour les anciennes lignes non appariées\n",
    "            for i_rel in old_unmatched:\n",
    "                old_content = old_block[i_rel]\n",
    "                if not old_content.strip():\n",
    "                    continue\n",
    "                old_abs_line = i1 + i_rel + 1\n",
    "                human_rows.append(f\"- Ancienne ligne {old_abs_line}. {old_content}\")\n",
    "                diff_json.append({\n",
    "                    \"type\": \"delete\",\n",
    "                    \"old_line\": old_abs_line,\n",
    "                    \"old_content\": old_content\n",
    "                })\n",
    "\n",
    "    human_str = \"\\n\".join(human_rows)\n",
    "    return human_str, diff_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9212b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"\"\"Confirmer la MNV SNCF Nantes-Vannes Étude RDCR Caen Cherbourg 225kV \n",
    "Analyse des audits Cherbourg\"\"\"\n",
    "\n",
    "text2 = \"\"\"Confirmé 1 MNV SNCF Nantes-Vannes \n",
    "Étude RDCR Gouin Cherbourg 225kV \n",
    "Analyse des audios Cherbourg\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f936e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Résumé humain du diff :\n",
      "\n",
      "~ Ligne 1. Confirmé 1 MNV SNCF Nantes-Vannes \n",
      "~ Ligne 2. Étude RDCR Gouin Cherbourg 225kV \n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test du diff avec seuil d'ignorance des changements mineurs\n",
    "human, diff = compute_diff(text1, text2, minor_change_threshold=0.9)\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Résumé humain du diff :\\n\")\n",
    "if human.strip():\n",
    "    print(human)\n",
    "else:\n",
    "    print(\"Aucune différence significative détectée.\")\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bd3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bbd63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 =\"\"\"Incident: coupure focale 140kV (poste PN-2) MV à faire toutes les 15 min\n",
    "Changement numéro M. Dupont\n",
    "+33663462732\n",
    "G +33664472833\n",
    "Appel à maintenance: confirmer bascule en cours\n",
    "Changement de poste\n",
    "Faire : Basculer sur le réseau RN19\n",
    "Appeler la maintenance sur place\n",
    "Prévoir une solution à l'augmentation du flux\"\"\"\n",
    "text2 =\"\"\"Incident: coupure focale 150kV (poste PN-2) MV à faire toutes les 15 min\n",
    "Changement numéro M. Dupont\n",
    "+33663462732 +38665572833\n",
    "Appel à maintenance: confirmer bascule en heures\n",
    "Changement de poste\n",
    "Faire : Basculer sur le réseau RN19\n",
    "Appeler la maintenance sur place\n",
    "Prévoir une solution à l'augmentation du flux\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf2afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1_corrgé = \"\"\"Incident: coupure focale 140kV (poste PN-2) MV à faire toutes les 15 min. \n",
    "Changement numéro M. Dupont.\n",
    "+33663462732\n",
    "G +33664472833\n",
    "Appel à maintenance: confirmer bascule en cours\n",
    "Changement de poste\n",
    "Faire : Basculer sur le réseau RN19\n",
    "Appeler la maintenance sur place\n",
    "Prévoir une solution à l'augmentation du flux\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5c2eb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incident: coupure focale 140kV (poste PN-2) MV à faire toutes les 15 min\n",
      "Changement numéro M. Dupont\n",
      "+33663462732\n",
      "G +33664472833\n",
      "Appel à maintenance: confirmer bascule en cours\n",
      "Changement de poste\n",
      "Faire : Basculer sur le réseau RN19\n",
      "Appeler la maintenance sur place\n",
      "Prévoir une solution à l'augmentation du flux\n",
      "[\"Incident: coupure focale 140kV (poste PN-2) MV à faire toutes les 15 min\\nChangement numéro M. Dupont\\n+33663462732\\nG +33664472833\\nAppel à maintenance: confirmer bascule en cours\\nChangement de poste\\nFaire : Basculer sur le réseau RN19\\nAppeler la maintenance sur place\\nPrévoir une solution à l'augmentation du flux\"]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(text1.strip())\n",
    "print(re.split(r\"\\n\\s*\\n\", text1.strip()))\n",
    "print(len(re.split(r\"\\n\\s*\\n\", text1.strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ab7dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflow_text(text: str, width: int = 80) -> str:\n",
    "    \"\"\"\n",
    "    Reflow paragraphs so each line has approximately `width` characters.\n",
    "\n",
    "    Behaviour:\n",
    "    - Split on blank lines to preserve paragraph boundaries.\n",
    "    - For each paragraph, collapse internal line breaks and multiple spaces,\n",
    "      then wrap to the requested width using a greedy algorithm that does\n",
    "      not break long words or hyphenated tokens.\n",
    "    - Return the reflowed text with original paragraph separations.\n",
    "\n",
    "    This will rearrange words across original line breaks to produce lines\n",
    "    of approximately equal length.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return text\n",
    "\n",
    "    paras = re.split(r\"\\n\\s*\\n\", text.strip())\n",
    "    out_paras: List[str] = []\n",
    "\n",
    "    for p in paras:\n",
    "        # collapse lines and whitespace inside a paragraph\n",
    "        single = \" \".join([ln.strip() for ln in p.splitlines() if ln.strip()])\n",
    "        single = re.sub(r\"\\s+\", \" \", single).strip()\n",
    "\n",
    "        # If the paragraph is short, keep it as-is\n",
    "        if len(single) <= width:\n",
    "            out_paras.append(single)\n",
    "            continue\n",
    "\n",
    "        wrapped = textwrap.fill(single, width=width, break_long_words=False, break_on_hyphens=False)\n",
    "        out_paras.append(wrapped)\n",
    "\n",
    "    return \"\\n\\n\".join(out_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5631e768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection-notes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
